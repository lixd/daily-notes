# IO复用

**模型一：单线程accept**

只能同时处理一个请求，后续请求只能阻塞



**模型二：单线程accept+多线程处理业务**

能同时处理多个请求了，但是每个请求开一个线程，对于高并发场景，线程数会受到硬件瓶颈，且线程过多会增加CPU的切换成本。

**模型三：单线程多路IO复用**

多路IO复用可以处理多个客户端，但是实际还是串行执行，会有阻塞。

**模型四：单线程多路IO复用+线程池处理业务**

在模型三基础上，将业务处理拆分出来由单独线程池处理，能够减少客户端的等待时间。

但是最高读写也只有1，因为读写操作还是由主线程在处理。

**模型五：单线程IO复用+多线程IO复用处理业务**

在模型三基础上，将主线程处理的读写分散到多个线程去处理，主线程只负责处理Connect请求，这样就增加了并行通道，具体并行数和多线程数一致。

同时多个线程进行多路IO复用监听，可以增加最大监听的FD数。

可以将线程池和CPU绑定，减少切换成本

缺点：最大并行数已经为N，同时处于一个线程中的客户端还是会出现等待情况。可以看做是模型三的并发版。

**模型五进程版：单进程多路IO复用+多进程多路IO复用**

和模型五没有太多差别，只是因为进程和线程在内存布局不同，主进程不再进行accept操作，而是将accept操作也分散到各个子进程中。

> 因为文件描述符不能在多进程间共享，所以需要由子进程自己执行accept

同时多进程会占用资源稍微多一些，然后各进程之间不会互相干扰，安全性好些。

**模型六：单线程多路IO复用+多线程多路IO复用+多线程处理业务**

在模型五基础上，解决了读写并行通道的问题，但是过于理想化，需要CPU的核心数够大才行。

如果CPU数不够，则会导致大量线程切换，浪费CPU。



**小结**：模型五为生产环境的大多数选择，模型六则过于理想化，当前硬件水平还达不到要求。