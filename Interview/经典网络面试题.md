# 经典网络面试题、

[3 万字 + 100 张图带你彻底搞懂 TCP 面试题（强烈建议收藏）](https://copyfuture.com/blogs-details/20210803020507301g)

## 其他问题

### 为什么是三次握手？不是两次、四次？

相信大家比较常回答的是：“因为三次握手才能保证双方具有接收和发送的能力。”

这回答是没问题，但这回答是片面的，并没有说出主要的原因。

在前面我们知道了什么是 **TCP 连接**：

- 用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括**Socket、序列号和窗口大小**称为连接。

所以，重要的是**为什么三次握手才可以初始化Socket、序列号和窗口大小并建立 TCP 连接。**

接下来以三个方面分析三次握手的原因：

- 三次握手才可以阻止重复历史连接的初始化（主要原因）
- 三次握手才可以同步双方的初始序列号
- 三次握手才可以避免资源浪费

客户端连续发送多次 SYN 建立连接的报文，在**网络拥堵**情况下：

- 如果一个「旧 SYN 报文」比「最新的 SYN 」 报文早到达了服务端；
- 那么此时服务端就会回一个 `SYN + ACK` 报文给客户端；
- 客户端收到后可以根据自身的上下文，判断这是一个历史连接（序列号过期或超时），那么客户端就会发送 `RST` 报文给服务端，表示中止这一次连接。

如果是两次握手连接，就不能判断当前连接是否是历史连接，三次握手则可以在客户端（发送方）准备发送第三次报文时，客户端因有足够的上下文来判断当前连接是否是历史连接：

- 如果是历史连接（序列号过期或超时），则第三次握手发送的报文是 `RST` 报文，以此中止历史连接；
- 如果不是历史连接，则第三次发送的报文是 `ACK` 报文，通信双方就会成功建立连接；

所以，TCP 使用三次握手建立连接的最主要原因是**防止历史连接初始化了连接。**

*原因二：同步双方初始序列号*

客户端发送携带「初始序列号」的 `SYN` 报文的时候，需要服务端回一个 `ACK` 应答报文，表示客户端的 SYN 报文已被服务端成功接收，那当服务端发送「初始序列号」给客户端的时候，依然也要得到客户端的应答回应，**这样一来一回，才能确保双方的初始序列号能被可靠的同步。**

*原因三：避免资源浪费*

如果只有「两次握手」，当客户端的 `SYN` 请求连接在网络中阻塞，客户端没有接收到 `ACK` 报文，就会重新发送 `SYN` ，由于没有第三次握手，服务器不清楚客户端是否收到了自己发送的建立连接的 `ACK` 确认信号，所以每收到一个 `SYN` 就只能先主动建立一个连接，这会造成什么情况呢？

如果客户端的 `SYN` 阻塞了，重复发送多次 `SYN` 报文，那么服务器在收到请求后就会**建立多个冗余的无效链接，造成不必要的资源浪费。**

不使用「两次握手」和「四次握手」的原因：

- 「两次握手」：无法防止历史连接的建立，会造成双方资源的浪费，也无法可靠的同步双方序列号；
- 「四次握手」：三次握手就已经理论上最少可靠连接建立，所以不需要使用更多的通信次数。

### 为什么客户端和服务端的初始序列号 ISN 是不相同的？

如果一个已经失效的连接被重用了，但是该旧连接的历史报文还残留在网络中，如果序列号相同，那么就无法分辨出该报文是不是历史报文，如果历史报文被新的连接接收了，则会产生数据错乱。

所以，每次建立连接前重新初始化一个序列号主要是为了通信双方能够根据序号将不属于本连接的报文段丢弃。

另一方面是为了安全性，防止黑客伪造的相同序列号的 TCP 报文被对方接收。

> 起始 `ISN` 是基于时钟的，每 4 毫秒 + 1，转一圈要 4.55 个小时。



### 既然 IP 层会分片，为什么 TCP 层还需要 MSS 呢？

- `MTU`：一个网络包的最大长度，以太网中一般为 `1500` 字节；
- `MSS`：除去 IP 和 TCP 头部之后，一个网络包所能容纳的 TCP 数据的最大长度；

如果一个网络包超过了 MTU限制就要拆分成多个。

如果是IP层进行拆分，比如一个TCP层数据包被拆分成了两个，其中一个丢失了，那么IP层就无法组装回去，也就不能提供给TCP层，而TCP层是可靠的，于是TCP层就要重传，于是这几个数据包都要再发一次，虽然可能只丢失了一个。

如果 TCP 层分片后，即使一个 TCP 分片丢失后，**进行重发时也是以 MSS 为单位**，而不用重传所有的分片，大大增加了重传的效率。



### SYN攻击

SYN 攻击：一直发送SYN，占满服务器的SYN队列，导致新用户无法建立连接。

解决方案：开启TCP Cookie.

```shell
net.ipv4.tcp_syncookies = 1
```

- 当 「 SYN 队列」满之后，后续服务器收到 SYN 包，不进入「 SYN 队列」；
- 计算出一个 `cookie` 值，再以 SYN + ACK 中的「序列号」返回客户端，
- 服务端接收到客户端的应答报文时，服务器会检查这个 ACK 包的合法性。如果合法，直接放入到「 Accept 队列」。
- 最后应用通过调用 `accpet()` socket 接口，从「 Accept 队列」取出的连接。



### 为什么挥手需要四次？

再来回顾下四次挥手双方发 `FIN` 包的过程，就能理解为什么需要四次了。

- 关闭连接时，客户端向服务端发送 `FIN` 时，仅仅表示客户端不再发送数据了但是还能接收数据。
- 服务器收到客户端的 `FIN` 报文时，先回一个 `ACK` 应答报文，而服务端可能还有数据需要处理和发送，等服务端不再发送数据时，才发送 `FIN` 报文给客户端来表示同意现在关闭连接。

从上面过程可知，服务端通常需要等待完成数据的发送和处理，所以服务端的 `ACK` 和 `FIN` 一般都会分开发送，从而比三次握手导致多了一次。



### 为什么 TIME_WAIT 等待的时间是 2MSL？

`MSL` 是 Maximum Segment Lifetime，**报文最大生存时间**，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。因为 TCP 报文基于是 IP 协议的，而 IP 头中有一个 `TTL` 字段，是 IP 数据报可以经过的最大路由数，每经过一个处理他的路由器此值就减 1，当此值为 0 则数据报将被丢弃，同时发送 ICMP 报文通知源主机。

MSL 与 TTL 的区别： MSL 的单位是时间，而 TTL 是经过路由跳数。所以 **MSL 应该要大于等于 TTL 消耗为 0 的时间**，以确保报文已被自然消亡。

TIME_WAIT 等待 2 倍的 MSL，比较合理的解释是： 网络中可能存在来自发送方的数据包，当这些发送方的数据包被接收方处理后又会向对方发送响应，所以**一来一回需要等待 2 倍的时间**。

比如如果被动关闭方没有收到断开连接的最后的 ACK 报文，就会触发超时重发 Fin 报文，另一方接收到 FIN 后，会重发 ACK 给被动关闭方， 一来一去正好 2 个 MSL。

`2MSL` 的时间是从**客户端接收到 FIN 后发送 ACK 开始计时的**。如果在 TIME-WAIT 时间内，因为客户端的 ACK 没有传输到服务端，客户端又接收到了服务端重发的 FIN 报文，那么 **2MSL 时间将重新计时**。

在 Linux 系统里 `2MSL` 默认是 `60` 秒，那么一个 `MSL` 也就是 `30` 秒。**Linux 系统停留在 TIME_WAIT 的时间为固定的 60 秒**。



### 为什么需要TIME_WAIT状态？

主动发起关闭连接的一方，才会有 `TIME-WAIT` 状态。

需要 TIME-WAIT 状态，主要是两个原因：

- 防止具有相同「四元组」的「旧」数据包被收到；
- 保证「被动关闭连接」的一方能被正确的关闭，即保证最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭；

*原因一：防止旧连接的数据包*

假设 TIME-WAIT 没有等待时间或时间过短，被延迟的数据包抵达后会发生什么呢？

服务端在关闭连接之前发送的  报文，被网络延迟了。然后没有TIME_WAIT状态，直接关闭连接。然后该端口被TCP 连接被复用后，被延迟的 报文抵达了客户端，那么客户端是有可能正常接收这个过期的报文，这就会产生数据错乱等严重的问题。

> 所以，TCP 就设计出了这么一个机制，经过 `2MSL` 这个时间，**足以让两个方向上的数据包都被丢弃，使得原来连接的数据包在网络中都自然消失，再出现的数据包一定都是新建立连接所产生的。**

*原因二：保证连接正确关闭*

也就是说，TIME-WAIT 作用是**等待足够的时间以确保最后的 ACK 能让被动关闭方接收，从而帮助其正常关闭。**

如果客户端四次挥手的最后一个 `ACK` 报文如果在网络中被丢失了，此时如果客户端 `TIME-WAIT` 过短或没有，则就直接进入了 `CLOSED` 状态了，那么服务端则会一直处在 `LASE_ACK` 状态。

> 因为服务端没有收到最后的ACK导致一直无法关闭。

如果 TIME-WAIT 等待足够长的情况就会遇到两种情况：

- 服务端正常收到四次挥手的最后一个 `ACK` 报文，则服务端正常关闭连接。
- 服务端没有收到四次挥手的最后一个 `ACK` 报文时，则会重发 `FIN` 关闭连接报文并等待新的 `ACK` 报文。

所以客户端在 `TIME-WAIT` 状态等待 `2MSL` 时间后，就可以**保证双方的连接都可以正常的关闭。**



### TIME_WAIT 过多有什么危害？

如果服务器有处于 TIME-WAIT 状态的 TCP，则说明是由服务器方主动发起的断开请求。

过多的 TIME-WAIT 状态主要的危害有两种：

- 第一是内存资源占用；
- 第二是对端口资源的占用，一个 TCP 连接至少消耗一个本地端口；

第二个危害是会造成严重的后果的，要知道，端口资源也是有限的，一般可以开启的端口为 `32768～61000`，也可以通过如下参数设置指定

```
net.ipv4.ip_local_port_range
```

**如果发起连接一方的 TIME_WAIT 状态过多，占满了所有端口资源，则会导致无法创建新连接。**

客户端受端口资源限制：

- 客户端TIME_WAIT过多，就会导致端口资源被占用，因为端口就65536个，被占满就会导致无法创建新的连接。

服务端受系统资源限制：

- 由于一个四元组表示 TCP 连接，理论上服务端可以建立很多连接，服务端确实只监听一个端口 但是会把连接扔给处理线程，所以理论上监听的端口可以继续监听。但是线程池处理不了那么多一直不断的连接了。所以当服务端出现大量 TIME_WAIT 时，系统资源被占满时，会导致处理不过来新的连接。

> 如何优化 TIME_WAIT？

这里给出优化 TIME-WAIT 的几个方式，都是有利有弊：

- 打开 net.ipv4.tcp_tw_reuse 和 net.ipv4.tcp_timestamps 选项；
- net.ipv4.tcp_max_tw_buckets
- 程序中使用 SO_LINGER ，应用强制使用 RST 关闭。

*方式一：net.ipv4.tcp_tw_reuse 和 tcp_timestamps*

如下的 Linux 内核参数开启后，则可以**复用处于 TIME_WAIT 的 socket 为新的连接所用**。

有一点需要注意的是，**tcp_tw_reuse 功能只能用客户端（连接发起方），因为开启了该功能，在调用 connect() 函数时，内核会随机找一个 time_wait 状态超过 1 秒的连接给新的连接复用。**

```
net.ipv4.tcp_tw_reuse = 1
```

使用这个选项，还有一个前提，需要打开对 TCP 时间戳的支持，即

```
net.ipv4.tcp_timestamps=1（默认即为 1）
```

这个时间戳的字段是在 TCP 头部的「选项」里，用于记录 TCP 发送方的当前时间戳和从对端接收到的最新时间戳。

由于引入了时间戳，我们在前面提到的 `2MSL` 问题就不复存在了，因为重复的数据包会因为时间戳过期被自然丢弃。

*方式二：net.ipv4.tcp_max_tw_buckets*

这个值默认为 18000，当系统中处于 TIME_WAIT 的连接**一旦超过这个值时，系统就会将后面的 TIME_WAIT 连接状态重置。**

这个方法过于暴力，而且治标不治本，带来的问题远比解决的问题多，不推荐使用。

*方式三：程序中使用 SO_LINGER*

我们可以通过设置 socket 选项，来设置调用 close 关闭连接行为。

```c
struct linger so_linger;
so_linger.l_onoff = 1;
so_linger.l_linger = 0;
setsockopt(s, SOL_SOCKET, SO_LINGER, &so_linger,sizeof(so_linger));
```

如果`l_onoff`为非 0， 且`l_linger`值为 0，那么调用`close`后，会立该发送一个`RST`标志给对端，该 TCP 连接将跳过四次挥手，也就跳过了`TIME_WAIT`状态，直接关闭。

但这为跨越`TIME_WAIT`状态提供了一个可能，不过是一个非常危险的行为，不值得提倡。





### 客户端调用 close 了，连接是断开的流程是什么？

- 客户端调用 `close`，表明客户端没有数据需要发送了，则此时会向服务端发送 FIN 报文，进入 FIN_WAIT_1 状态；
- 服务端接收到了 FIN 报文，TCP 协议栈会为 FIN 包插入一个文件结束符 `EOF` 到接收缓冲区中，应用程序可以通过 `read` 调用来感知这个 FIN 包。这个 `EOF` 会被**放在已排队等候的其他已接收的数据之后**，这就意味着服务端需要处理这种异常情况，因为 EOF 表示在该连接上再无额外数据到达。此时，服务端进入 CLOSE_WAIT 状态；
- 接着，当处理完数据后，自然就会读到 `EOF`，于是也调用 `close` 关闭它的套接字，这会使得客户端会发出一个 FIN 包，之后处于 LAST_ACK 状态；
- 客户端接收到服务端的 FIN 包，并发送 ACK 确认包给服务端，此时客户端将进入 TIME_WAIT 状态；
- 服务端收到 ACK 确认包后，就进入了最后的 CLOSE 状态；
- 客户端经过 `2MSL` 时间之后，也进入 CLOSE 状态；



## 2. 面试题

### 1.TCP Listen 时的backlog参数意义？

```c
int sockfd =socket()
bind(sockfd)
listen(sockfd,backlog)
```

TCP 需要三次握手，

第一次握手之后，服务端需要把客户端数据暂存起来，这个叫做 **半连接队列**，也叫 syn 队列。

三次握手完成后成功建立连接，就会被放到 **全连接队列**，也叫 accept 队列。

backlog 就指的是 accept 队列的长度。



### 2. accept 发生在三次握手的哪一步？

accept 发生在三次握手之后。

accept 实际是从全连接队列中取出一个连接并为其分配 fd。



### 3. tcp和udp的区别

学术上的：

* tcp是面向流的，流式套接字
* udp是面向数据包的

什么是TCP?
TCP（Transmission ControlProtocol，传输控制协议）
1.它是传输层的一个协议
2.它是一个面向连接的协议，连接前必须经历“三次握手”（我们可以理解为打电话，只有电话通了，两个人才可以说话）
3.它是一种可靠传输
4.面向字节流（可以理解为像水一样传输数据）

5.TCP发送的数据是有序的

6.TCP有粘包问题

7.TCP传输效率低

8.若通信数据完整性需让位与通信实时性，则应该选用 TCP 协议（如文件传输、重要状态的更新等）；则使用 UDP 协议（如视频传输、实时通信等）。
UDP（User Datagram Prototocol 用户数据报协议）
1.它是传输层的一个协议
2.它是一个无连接的协议（我们可以理解为发短信，不需要建立连接就可以发送数据）
3.它是一种不可靠传输（因为它面向无连接，所以必然是一种不可靠的传输）
4.面向数据报（可以理解为像冰块一样一块一块发送数据）





流和数据包的区别？：流是应用层的概念，不是TCP中的概念。个人理解就是流中的每个数据包是有关系的，使得这些数据包形成了一种流，比如TCP中的数据包都是有顺序的，而UDP数据包之间没有顺序，所以UDP就只是面向数据包的。

实际一点的：

* udp实时性较强，游戏、直播等领域用的比较多

因为TCP要保证数据包的有序性，但是网络是不可控的，即数据包发送出去后具体走什么路线完全有路数的路由器、网关等路由表决定，而这些路由信息又可能会改变，所以不存在先发送的数据包就会先到的说法。

IP层不能保证顺序，但是TCP需要对应用层负责，因此TCP只能自己保证顺序。

而**保证顺序的唯一方法就是排队**，所以TCP实时性会低一些。



* UDP没有拥塞控制
  * TCP有滑动窗口

这也就是为什么，P2P下载会导致整个局域网的网络都非常慢。







udp server fd是否针对大量客户端同时发送？

采用模拟TCP三次握手的方式来解决该问题。

第一次收到客户端sendto发送的数据后，udp server会为该客户端创建一个fd，并且从另外一个端口再把数据发送给客户端，这样该客户端和udpserver就在单独的端口进行通信了。

有没有可能出现脏数据？

> 一个数据包里同时包含多个客户端的数据



### 4. 大量close_wait原因？

四次挥手。

* 主动方：fin
* 被动方：ack
* 被动方：fin
* 主动方：ack

第二步中，被动方收到fin并且回复ack只会，就进入了 close_wait状态。

大部分情况下都是服务端作为被动方，由客户端主动关闭。

服务端怎么判断对方发送了fin，关闭连接呢？

```c
ret = recv(fd,)
if ret == 0 { // 为0说明收到了客户端的fin，然后第二步的ack是tcp自动发送的，程序里不用管
    
    // 关闭连接之前，在这里可能有一个其他逻辑处理
    
    // 此时服务端就需要主动断开连接，执行第三步，给客户端也回一个fin 
    close(fd);// 这个close就是第三步，给客户端也回一个fin
}

```

大量 close_wait 可能就是收到fin和服务端调用 close方法这中间的时间比较长，导致连接一直处于 close_wait 状态。

比如IM系统中，客户端发送fin只后，服务端可能需要做大量清理工作，必须清除客户端在线状态，移除临时数据等等。

如何优化？

把业务逻辑从连接管理中移除，比如改成异步逻辑。



### 5.closeing的原因

closeing状态很少见，只有当双方同时关闭连接的时候才会进入到这个状态。



### 6.eagain的原因

```c
int nready = epoll_wait();
for(i=0;i < nread;i++;){
    if(epollin){
       int ret== recv();
        if(ret>0){
            // 读取到数据
        }else if(ret==0){
            // 断开连接
        }else{ // 判断到是eagain错误码直接continue
            if(errno==eagain){
                continue;
            }
        }
    }else if(epollout){
        
    }
}
```

eagain一般是在边沿触发ET的时候出现。

epoll只触发了一次，然后程序循环去读，就会出现这个问题，循环recv直到没有数据了，就返回-1 eagain。

LT则是有数据就会一直触发，所以程序只需要recv一次就行，没读完的等下次触发在读。

正常情况下LT不会出现eagain，如果是多线程同时recv一个fd则有可能出现。





### 7.TCP如何保证顺序

可靠通信必须实现以下三原则：

* 1）不丢包
* 2）不重复
* 3）完整性(原子性)

为了达到这三项要求，对应有**三条定理(可靠通信三原则)**：

* 1）**重传定理**：确认和重传是解决丢包问题的**唯一正确**方法
* 2）**去重定理**：排队是解决去重问题的**唯一正确**方案
* 3）**原子定理**：单点标记或者循环自校验是实现完整性的**唯一正确**方法。



> 有些同学可能对排队理论有怀疑, 表示搞一个全局位图(标记集合)也能解决去重问题. 如果深究, 判断标记和修改标记是独立的两步操作, 这两步操作就遇到去重问题, 也即, 对标记集合的操作本身也需要排队. 当然, CAS 是一种解决方案, 但 CAS 的实现内部本质也是排队.





TCP 为什么能实现可靠通信？

因为 TCP 至少遵循了这三条定理：

* TCP 有超时重传机制兜底(还有选择重传、快速重传等多项技术进行优化)
* TCP 的序号用于排队
* TCP 有 checksum 校验数据的完整性





### 8.Epoll的惊群如何解决？

何为惊群？

简单理解：多个线程一直等待某个条件满足就执行某项任务，当条件满足时所有线程都会被唤醒。

> 实际上只需要唤醒一个线程就行了



> 主进程首先调用listen创建监听TCP套接字，进行监听，接着调用epoll_create创建一个epfd和对应的红黑树，然后调用fork创建多进程，子进程内部调用epoll_wait等待同一个epfd的事件，当有TCP连接请求到达时，也就是这个epfd有事件发生时，这些子进程全部被唤醒并处理事件，只有一个子进程通过调用非阻塞accept抢占到该连接请求并创建TCP连接，然后进行handle。
>
> 这种方式有如下2个缺点：
>
> - 同样会导致惊群效应
> - 由于等待的是同一个epfd，假如A进程创建了TCP连接A，B进程创建了TCP连接B，当连接A发生读写事件时，可能B进程抢占了该事件，导致混乱。



如何解决？

保证只把 fd传递给一个线程，只在一个epoll里，从根源上解决问题。



Nginx惊群

Nginx实现

> 主进程首先调用listen进行监听，接着调用fork创建多进程，子进程内部调用epoll_create创建各自的epfd和各自的红黑树，并调用epoll_wait等待同各自epfd的事件，这些子进程都将listen端口的监听事件放在自己的epfd中，当有TCP连接请求到达时，每个子进程的epfd的这个事件都会发生，这些子进程全部被唤醒并处理各自的这个事件，只有一个子进程通过调用非阻塞accept抢占到该连接请求并创建TCP连接，然后进行handle。
>
> 这种方式对比于第3种方式，不会出现进程与连接的不对应（创建TCP连接后的子进程会将该连接放在自己的epfd中而不是公共的epfd中），但在监听事件上仍然会出现惊群效应。

**锁策略**

基本思路是，子进程在进行epoll_ctl加入监听事件以及epoll_wait前，需要获得进程间的全局锁，获得锁的子进程才有资格通过监听获取连接请求并创建TCP连接，锁策略确保了只有1个子进程在处理TCP连接请求。

**SO_REUSEPORT**

在Linux 3.9版本引入了socket套接字选项SO_REUSEPORT.，Linux 3.9版本之前，一个进程通过bind一个三元组（{<protocol>, <src_addr>, <src_port>}）组合之后，其他进程不能再bind同样的三元组，Linux 3.9版本之后，凡是传入选项SO_REUSEPORT且为同一个用户下（安全考虑）的socket套接字都可以bind和监听同样的三元组。

内核对这些监听相同三元组的socket套接字实行负载均衡，将TCP连接请求均匀地分配给这些socket套接字。

> 这里的负载均衡基本原理为：当有TCP连接请求到来时，用数据包的（{<src_addr>, <src_port>}）作为一个hash函数的输入，将hash后的结果对SO_REUSEPORT套接字的数量取模，得到一个索引，该索引指示的数组位置对应的套接字便是要处理连接请求的套接字。



**EPOLLEXCLUSIVE标志位**

在Linux 4.5版本引入EPOLLEXCLUSIVE标志位（Linux 4.5, glibc 2.24），子进程通过调用epoll_ctl将监听套接字与监听事件加入epfd时，会同时将EPOLLEXCLUSIVE标志位显式传入，这使得子进程带上了exclusive属性，也就是互斥属性，跟Linux 2.6版本解决accept惊群效应的解决方案类似，不同的地方在于，当有监听事件发生时，唤醒的可能不止一个进程（见如下对EPOLLEXCLUSIVE标志位的官方文档说明中的“one or more”），这一定程度上缓解了惊群效应。

**Nginx实现**

前面提到内核解决epoll的惊群效应是比较晚的，因此nginx自身解决了该问题（更准确的说是避免了）。其具体思路是：**不让多个进程在同一时间监听接受连接的socket，而是让每个进程轮流监听**，这样当有连接过来的时候，就只有一个进程在监听那肯定就没有惊群的问题。具体做法是：利用一把进程间锁，每个进程中都**尝试**获得这把锁，如果获取成功将监听socket加入wait集合中，并设置超时等待连接到来，没有获得所的进程则将监听socket从wait集合去除。
