# 云原生技术体系

> http://gk.link/a/10tN8
>
> https://www.bilibili.com/video/BV1JK411j7X7/



## 阶段一:从单体应用到企业服务总线



* 业务架构:架构耦合问题，架构腐化问题，技术债务问题
* 技术架构:资源申请慢，复用性差，高可用性差
* 组织架构:研发运维标准不一，难保障端到端高可用



*架构为什么会不断腐化?*

* 没时间搞
  * 因不可观测，领导重视功能和Bug,不重视架构，不给留足够时间

* 没动力搞
  * 代码可理解性差，反正CodeReview不出来，所以头疼医头

* 没胆量搞
  * 代付复杂，耦合性高，核心逻辑不敢动，只好层层封装

架构腐化的影响

* 迭代速度慢
  * 你改代码，你要上线，要我配合
  * 明明改的模块 A，最后发现可能牵扯到了模块 BCD。

* 可复用性差
  * 明明有某个功能，却拿不出来

架构腐化的最终结果

* 技术债务
  * 一个功能原来需要开发一下午，现在需要开发一个月

### 小结

阶段一有问题吗?其实没有问题。

* 中间件，服务层，前端，全部由外包商或者乙方搞定，端到端维护。
* 数据库Oracle, DB2，SQL Server性能保证，存储过程简化开发、
* 开发运维界限清楚，保障安全

  什么时候觉得阶段一有问题?

* 当系统需要灵活的响应业务变化的时候，才会感觉到痛。



## 阶段二:服务化加速业务创新



* 业务架构: 架构服务化，侧重变化多和复用性，领域拆分与解耦

* 技术架构:基础设施云化，统一接口，抽象概念，租户自助

* 研发流程:发布模式平台化，构建持续集成流程，质量和绩效看板

  





* 可理解性
  * 工程简洁，职责单一，易修改，易Review
* 可测试性
  * 单元测试覆盖度高，集成测试容易
* 可观测性
  * 成立架构委员会，制定规范，实时监测



服务化后的好处

* 快速迭代
  * 开发敢改，同事易review,QA易测试，领导易看到价值
* 可复用性



### 规划

> DDD 领域驱动设计

* (1)梳理核心业务流程
* (2)划分核心业务领域
* (3)确定界限上下文及相互关系
* (4)输出按照领域横向拆分架构



### 试点

> 先构建自动化测试流程，并选择一部分业务来做试点，不建议一下子全部拆分

* (5)构建持续集成流程和测试集合
* (6)选取试点业务，横向拆分
  * 随着新需求的不断到来，渐进的进行拆分
    * 比如已经想好了模块A如何拆分，然后刚好来了一个模块A的需求，就可以借机将模块A从整个单体应用中拆分出来。
  * 变化多，复用性是两大考虑要素
  * 三次重构原则
    * 复杂逻辑不要想着一步到位，不可能一次性重构完，可以分多次重构



* (7)需要注册中心及API规范与知识库
  * 仅仅注册还不够，别忘了咱们的使命
  * 可观测性:接口是否符合规范，架构是否腐化，架构委员会有没有一个地方可以集中Review
  * 可复用性:将来谁想用某个接口，文档还是找人
  * 制定API接口规范，统一API知识库
  * 接口文档统一维护，文档与运行时一致，减少沟通成本
  * 如何一致呢?架构委员会制定流程，在持续集成中增加代码review和文档review的流程

* (8)为保证平滑拆分，前端无感知，配备API网关
  * 当一个服务拆分称为多个服务的时候，API网关对前端应用屏蔽服务拆分， 前端无感知
  * 当一个服务新从单体应用中拆分出来的时候，为了稳定性，API网关提供灰度能力

* (9)微服务拆分的渐进式技术方案

* (10)为了解耦和质量属性，纵向分层拆分
  * controller 层:实现特定场景的业务逻辑
  * 组合层 compose:组合多次调用，实现复杂的业务逻辑，封装公共业务逻辑
  * 原子层 generic:将数据库缓存操作封装在这一层，提供原子化接口

* (11)试点业务拆分完毕，总结服务化规范
* (12)如何保证规范落地，质量看板，流程保障，绩效考核



### 服务化

> 试点完成后，已经有一个业务组跑完了整个服务化的流程，此时可以进行其他组的服务化了

* (13)架构委员会的组织服务化分组，分组拆分，各组制定里程碑计划，定时向架构委员会汇报



### 小结

阶段二有问题吗?没啥问题!

* 架构服务化，可复用
* 持续集成，快速迭代
* 中间件PaaS化
* 基础设施云化

什么情况下觉得阶段二有问题?

* 高并发问题



## 阶段三:微服务化承载高并发流量



* 业务架构:架构微服务化，侧重服务治理能力
* 组织架构:研发和运维融合，应用交付提前到开发，应用治理下沉到运维





流量限速，过载保护:当外部请求超过预估值，及时保护

负载均衡: 100个副本轮流服务

防服务雪崩:不会一个服务挂 了流量压倒一片

防请求堆积:不会个慢，拖累整个链路都慢

弹性伸缩: 10个副本扛不住，改一个数字即可变为100个

### 微服务化

* (16)为支撑高并发，进一步拆分，性能优化
  * 订单状态流转独立，搜索独立

* (17)微服务拆分数目多，运维受到压力，容器化
  * 环境交付提前，Dev和Ops的融合
* (18)微服务拆分数目多，统一微服务框架，服务治理，防止雪崩和请求堆积
* (19)微服务拆分数目多，定位问题难，全链路监控
* (20) GitOps,一切皆代码
  * 代码是代码，配置是代码单实例运行环境，Dockerfile是代码，多实例运行环境编排文件是代码
* (21)多服务致性，使用TCC和事务消息
* (22)容器化之后，测试环境多，流量染色
  * 整个测试环境共享一套基准环境，部署所有应用。
  * API网关层进行流量染色。
  * NSF Agent携带染色消息，并且染色在调用链上持续传递(小环境调用到基准环境后，还能路由回到小环境) ,按照同环境优先的策略进行路由和消费。
  * 若分支环境缺失相关应用，则路由到基准环境或择由基准换消费。
* (23)基于流量染色的灰度发布，快速迭代
* (24)全链路压测，承载高并发
* (25)多机房高可用与单元化



