# K8s 日志收集

## 1. 概述

在K8S的官方文档中也介绍了多种的[日志收集方案](https://kubernetes.io/docs/concepts/cluster-administration/logging/)，总结起来主要有下述3种：原生方式、DaemonSet方式和Sidecar方式。

- 原生方式：使用 `kubectl logs` 直接在查看本地保留的日志，或者通过docker engine的 `log driver` 把日志重定向到文件、syslog、fluentd等系统中。
- DaemonSet方式：在K8S的每个node上部署日志agent，由agent采集所有容器的日志到服务端。
- Sidecar方式：一个POD中运行一个sidecar的日志agent容器，用于采集该POD主容器产生的日志。



总结下来：

- 原生方式相对功能太弱，一般不建议在生产系统中使用，否则问题调查、数据统计等工作很难完成；
- DaemonSet方式在每个节点只允许一个日志agent，相对资源占用要小很多，但扩展性、租户隔离性受限，比较适用于功能单一或业务不是很多的集群；
- Sidecar方式为每个POD单独部署日志agent，相对资源占用较多，但灵活性以及多租户隔离性较强，建议大型的K8S集群或作为PAAS平台为多个业务方服务的集群使用该方式。



DaemonSet  方式比较简单，而且适合更加适合微服务化，当然，并不是完美的。

Sidecar  也有不完美的地方，每个 pod 里都要存在一个日志收集的 agen 实在是太消耗资源了，而且很多问题也难以解决，比如：主容器挂了，agent还没收集完，就把它给kill掉，这个时候日志怎么处理，业务会不会受到要杀掉才能启动新的这一短暂过程的影响等。

**一般建议首选 DaemonSet  模式。**



## 2. 部署ELK

```yaml
kind: List
apiVersion: v1
items:
- apiVersion: apps/v1beta1
  kind: Deployment
  metadata:
    name: elk-single
  spec:
    replicas: 1
    template:
      metadata:
        name: kb-single
        labels:
          app: kb-single
      spec:
        containers:
        - image: docker.elastic.co/kibana/kibana:7.8.0
          name: kibana
          env:
          - name: ELASTICSEARCH_URL
            value: "http://elk-es:9200"
          ports:
          - name: http
            containerPort: 5601
- apiVersion: v1
  kind: Service
  metadata:
    name: kb-single-svc
  spec:
    type: NodePort
    ports:
    - name: http
      port: 5601
      targetPort: 5601 
      nodePort: 32601
    selector:
      app: kb-single            
- apiVersion: apps/v1beta1
  kind: Deployment
  metadata:
    name: es-single
  spec:
    replicas: 1
    template:
      metadata:
        name: es-single
        labels:
          app: es-single
      spec:
        containers:
        - image: docker.elastic.co/elasticsearch/elasticsearch:6.4.0
          name: es
          env:
          - name: network.host
            value: "_site_"
          - name: node.name
            value: "${HOSTNAME}"
          - name: discovery.zen.ping.unicast.hosts
            value: "${ES_SINGLE_NODEPORT_SERVICE_HOST}"
          - name: cluster.name
            value: "test-single"
          - name: ES_JAVA_OPTS
            value: "-Xms128m -Xmx128m"
          volumeMounts:
          - name: es-single-data
            mountPath: /usr/share/elasticsearch/data
        volumes:
          - name: es-single-data
            emptyDir: {}
- apiVersion: v1
  kind: Service
  metadata: 
    name: es-single-nodeport
  spec:
    type: NodePort
    ports:
    - name: http
      port: 9200
      targetPort: 9200
      nodePort: 31200
    - name: tcp
      port: 9300
      targetPort: 9300
      nodePort: 31300
    selector:
      app: es-single
- apiVersion: v1
  kind: Service
  metadata:
    name: es-single
  spec:
    clusterIP: None
    ports:
    - name: http
      port: 9200
    - name: tcp
      port: 9300
    selector:
      app: es-single
```



## 3. Filebeat

```yml
kind: List
apiVersion: v1
items:
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: filebeat-config
    labels:
      k8s-app: filebeat
      kubernetes.io/cluster-service: "true"
      app: filebeat-config
  data:
    filebeat.yml: |
      processors:
        - add_cloud_metadata:
      filebeat.modules:
      - module: system
      filebeat.inputs:
      - type: log
        paths:
          - /var/log/containers/*.log
        symlinks: true
        # json.message_key: log
        # json.keys_under_root: true
      output.elasticsearch:
        hosts: ['es-single:9200']
      logging.level: info        
- apiVersion: extensions/v1beta1
  kind: DaemonSet 
  metadata:
    name: filebeat
    labels:
      k8s-app: filebeat
      kubernetes.io/cluster-service: "true"
  spec:
    template:
      metadata:
        name: filebeat
        labels:
          app: filebeat
          k8s-app: filebeat
          kubernetes.io/cluster-service: "true"
      spec:
        containers:
        - image: docker.elastic.co/beats/filebeat:6.4.0
          name: filebeat
          args: [
            "-c", "/home/filebeat-config/filebeat.yml",
            "-e",
          ]
          securityContext:
            runAsUser: 0
          volumeMounts:
          - name: filebeat-storage
            mountPath: /var/log/containers
          - name: varlogpods
            mountPath: /var/log/pods
          - name: varlibdockercontainers
            mountPath: /var/lib/docker/containers
          - name: "filebeat-volume"
            mountPath: "/home/filebeat-config"
        nodeSelector:
          role: front
        volumes:
          - name: filebeat-storage
            hostPath:
              path: /var/log/containers
          - name: varlogpods
            hostPath:
              path: /var/log/pods
          - name: varlibdockercontainers
            hostPath:
              path: /var/lib/docker/containers
          - name: filebeat-volume
            configMap:
              name: filebeat-config
- apiVersion: rbac.authorization.k8s.io/v1beta1
  kind: ClusterRoleBinding
  metadata:
    name: filebeat
  subjects:
  - kind: ServiceAccount
    name: filebeat
    namespace: default
  roleRef:
    kind: ClusterRole
    name: filebeat
    apiGroup: rbac.authorization.k8s.io
- apiVersion: rbac.authorization.k8s.io/v1beta1
  kind: ClusterRole
  metadata:
    name: filebeat
    labels:
      k8s-app: filebeat
  rules:
  - apiGroups: [""] # "" indicates the core API group
    resources:
    - namespaces
    - pods
    verbs:
    - get
    - watch
    - list
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    name: filebeat
    namespace: default
    labels:
      k8s-app: filebeat
```







