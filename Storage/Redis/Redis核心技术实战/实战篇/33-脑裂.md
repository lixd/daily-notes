# 脑裂：一次奇怪的数据丢失

## 1. 概述

**所谓的脑裂，就是指在主从集群中，同时有两个主节点，它们都能接收写请求**。而脑裂最直接的影响，就是客户端不知道应该往哪个主节点写入数据，结果就是不同的客户端会往不同的主节点上写入数据。而且，严重的话，脑裂会进一步导致数据丢失。

> 比如客户端先往主节点A写入一条数据，然后第二次请求查询的时候发到了主节点B，发现刚才写入的数据没了。



## 2. 原因分析

当然，并不是所有的丢数据都是由脑裂导致的，数据同步出现问题也会导致丢数据。

### 数据同步

在主从集群中发生数据丢失，最常见的原因就是**主库的数据还没有同步到从库，结果主库发生了故障，等从库升级为主库后，未同步的数据就丢失了**。

可以通过`比对主从库上的复制进度差值`来进行判断，如果从库上的 `slave_repl_offset` 小于原主库的 `master_repl_offset`，那么可以认定数据丢失是由数据同步未完成导致的。



### 脑裂

**原主库假故障**导致的脑裂也会导致数据丢失。

具体流程如下：

* 1）主库由于网络或其他原因导致无法响应哨兵的心跳检测
* 2）哨兵判定主库下线，开始主从切换
* 3）**主从切换过程中**主库又恢复正常，此时客户端依旧可以往主库写入数据
* 4）主从同步完成，主库被降级为从库，从库则升级为新主库，接收客户端读写请求
* 5）此时步骤3中写入的数据全部丢失



## 3. 如何应对

Redis 已经提供了两个配置项来限制主库的请求处理，分别是 `min-slaves-to-write` 和 `min-slaves-max-lag`。

* min-slaves-to-write：这个配置项设置了主库能进行数据同步的最少从库数量；
* min-slaves-max-lag：这个配置项设置了主从库间进行数据复制时，从库给主库发送 ACK 消息的最大延迟（以秒为单位）。



我们可以把 min-slaves-to-write 和 min-slaves-max-lag 这两个配置项搭配起来使用，分别给它们设置一定的阈值，假设为 N 和 T。

这两个配置项组合后的要求是，主库连接的从库中至少有 N 个从库，和主库进行数据复制时的 ACK 消息延迟不能超过 T 秒，否则，主库就不会再接收客户端的请求了。

即使原主库是假故障，它在假故障期间也无法响应哨兵心跳，也不能和从库进行同步，自然也就无法和从库进行 ACK 确认了。这样一来，min-slaves-to-write 和 min-slaves-max-lag 的组合要求就无法得到满足，原主库就会被限制接收客户端请求，客户端也就不能在原主库中写入新数据了。

等到新主库上线时，就只有新主库能接收和处理客户端请求，此时，新写的数据会被直接写到新主库中。而原主库会被哨兵降为从库，即使它的数据被清空了，也不会有新数据丢失。



从 CAP 理论的角度来分析：

* 1）redis 集群允许脑裂存在，其实是一种可用性高的特征，但不保证数据一直。
* 2）redis 通过设置两个参数，一定程度上其实是在降低可用性，以提供数据一致性。
* 3）为什么愿意降低可用性？因为那部分的数据会因为主从切换而丢失，所以宁愿不可用。

通过提供 min-slaves-to-write 和 min-slaves-max-lag 这两个参数，让使用者可以根据具体场景在 A 和 C 之间进行选择。



## 4. 小结

数据丢失：

* 1）主从同步延迟情况下，进行主从切换导致丢数据
* 2）脑裂导致丢数据



脑裂发生的原因主要是原主库发生了假故障，我们来总结下假故障的两个原因。

* 1）和主库部署在同一台服务器上的其他程序临时占用了大量资源（例如 CPU 资源），导致主库资源使用受限，短时间内无法响应心跳。其它程序不再使用资源时，主库又恢复正常。
* 2）主库自身遇到了阻塞的情况，例如，处理 bigkey 或是发生内存 swap，短时间内无法响应心跳，等主库阻塞解除后，又恢复正常的请求处理了。

为了应对脑裂，你可以在主从集群部署时，通过合理地配置参数 min-slaves-to-write 和 min-slaves-max-lag，来预防脑裂的发生。

> 建议：假设从库有 K 个，可以将 min-slaves-to-write 设置为 K/2+1（如果 K 等于 1，就设为 1），将 min-slaves-max-lag 设置为十几秒（例如 10～20s），在这个配置下，如果有一半以上的从库和主库进行的 ACK 消息延迟超过十几秒，我们就禁止主库接收客户端写请求。